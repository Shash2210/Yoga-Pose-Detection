{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4247c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2187f24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking image integrity in training data...\n",
      "Found 1080 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Path to your training data directory\n",
    "train_dir = r'C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\TRAIN'\n",
    "\n",
    "# Check image integrity\n",
    "print(\"Checking image integrity in training data...\")\n",
    "for root, dirs, files in os.walk(train_dir):\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            image_path = os.path.join(root, file)\n",
    "            img = Image.open(image_path)\n",
    "            img.load()\n",
    "        except (OSError, IOError) as e:\n",
    "            # If an error occurs, print the filename and error message\n",
    "            print(f\"Error opening {image_path}: {e}\")\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "yoga_model = Sequential()\n",
    "yoga_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "yoga_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Dropout(0.25))\n",
    "yoga_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Dropout(0.25))\n",
    "yoga_model.add(Flatten())\n",
    "yoga_model.add(Dense(1024, activation='relu'))\n",
    "yoga_model.add(Dropout(0.5))\n",
    "yoga_model.add(Dense(5, activation='softmax'))  # Modified to have 5 units\n",
    "\n",
    "# Model compilation\n",
    "yoga_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaaaee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 4/16 [======>.......................] - ETA: 10s - loss: 1.6254 - accuracy: 0.1758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 30s 2s/step - loss: 1.6031 - accuracy: 0.2274 - val_loss: 1.5913 - val_accuracy: 0.2480\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 31s 2s/step - loss: 1.5878 - accuracy: 0.2411 - val_loss: 1.5888 - val_accuracy: 0.2598\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.5911 - accuracy: 0.2667 - val_loss: 1.5867 - val_accuracy: 0.2617\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.5854 - accuracy: 0.2657 - val_loss: 1.5757 - val_accuracy: 0.3604\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.5663 - accuracy: 0.3022 - val_loss: 1.5553 - val_accuracy: 0.4053\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 28s 2s/step - loss: 1.5384 - accuracy: 0.3691 - val_loss: 1.5129 - val_accuracy: 0.4326\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.4756 - accuracy: 0.4134 - val_loss: 1.4030 - val_accuracy: 0.4727\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 31s 2s/step - loss: 1.3667 - accuracy: 0.4498 - val_loss: 1.2976 - val_accuracy: 0.4893\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 34s 2s/step - loss: 1.3041 - accuracy: 0.4872 - val_loss: 1.2229 - val_accuracy: 0.5342\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 37s 2s/step - loss: 1.2641 - accuracy: 0.5030 - val_loss: 1.1915 - val_accuracy: 0.5439\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 37s 2s/step - loss: 1.2184 - accuracy: 0.5000 - val_loss: 1.1833 - val_accuracy: 0.5518\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 33s 2s/step - loss: 1.1914 - accuracy: 0.5531 - val_loss: 1.1142 - val_accuracy: 0.6025\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 37s 2s/step - loss: 1.1549 - accuracy: 0.5699 - val_loss: 1.0891 - val_accuracy: 0.6064\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 34s 2s/step - loss: 1.1344 - accuracy: 0.5650 - val_loss: 1.0741 - val_accuracy: 0.6230\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 32s 2s/step - loss: 1.0830 - accuracy: 0.5768 - val_loss: 1.0294 - val_accuracy: 0.6338\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 33s 2s/step - loss: 1.0764 - accuracy: 0.5915 - val_loss: 1.0070 - val_accuracy: 0.6406\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 32s 2s/step - loss: 1.0356 - accuracy: 0.6230 - val_loss: 0.9677 - val_accuracy: 0.6396\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 38s 2s/step - loss: 1.0130 - accuracy: 0.6319 - val_loss: 0.9268 - val_accuracy: 0.6777\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 57s 4s/step - loss: 0.9864 - accuracy: 0.6348 - val_loss: 0.9095 - val_accuracy: 0.6992\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 42s 3s/step - loss: 0.9394 - accuracy: 0.6644 - val_loss: 0.8597 - val_accuracy: 0.7090\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 42s 3s/step - loss: 0.9066 - accuracy: 0.6732 - val_loss: 0.8349 - val_accuracy: 0.7236\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 43s 3s/step - loss: 0.8982 - accuracy: 0.6772 - val_loss: 0.8048 - val_accuracy: 0.7217\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 42s 3s/step - loss: 0.8758 - accuracy: 0.6890 - val_loss: 0.7754 - val_accuracy: 0.7324\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 47s 3s/step - loss: 0.8430 - accuracy: 0.6959 - val_loss: 0.7521 - val_accuracy: 0.7471\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.8302 - accuracy: 0.6909 - val_loss: 0.7296 - val_accuracy: 0.7480\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 39s 3s/step - loss: 0.8143 - accuracy: 0.6969 - val_loss: 0.6938 - val_accuracy: 0.7539\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 42s 3s/step - loss: 0.7565 - accuracy: 0.7352 - val_loss: 0.6610 - val_accuracy: 0.7705\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 41s 3s/step - loss: 0.7385 - accuracy: 0.7313 - val_loss: 0.6229 - val_accuracy: 0.8018\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 41s 3s/step - loss: 0.7249 - accuracy: 0.7402 - val_loss: 0.6208 - val_accuracy: 0.7979\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.6790 - accuracy: 0.7589 - val_loss: 0.5640 - val_accuracy: 0.7949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "yoga_model_info = yoga_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=(1081 // 64),\n",
    "    epochs=30,\n",
    "    validation_data=train_generator,\n",
    "    validation_steps=(1081 // 64)\n",
    ")\n",
    "\n",
    "# Saving model weights\n",
    "yoga_model.save_weights('yoga_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b006ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 10, 10, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2343557 (8.94 MB)\n",
      "Trainable params: 2343557 (8.94 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "yoga_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2712c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import threading\n",
    "\n",
    "# Load the trained model for yoga pose classification\n",
    "yoga_model = Sequential()\n",
    "yoga_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "yoga_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Dropout(0.25))\n",
    "yoga_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Dropout(0.25))\n",
    "yoga_model.add(Flatten())\n",
    "yoga_model.add(Dense(1024, activation='relu'))\n",
    "yoga_model.add(Dropout(0.5))\n",
    "yoga_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "yoga_model.load_weights('yoga_model.h5')  # Load pre-trained weights\n",
    "\n",
    "pose_dict = {0: \"downdog\", 1: \"goddess\", 2: \"plank\", 3: \"tree\", 4: \"warrior2\"}\n",
    "\n",
    "pose_images = {\n",
    "    \"downdog\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\dd.png\",\n",
    "    \"goddess\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\gd.png\",\n",
    "    \"plank\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\plank.jpg\",\n",
    "    \"tree\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\treee.png\",\n",
    "    \"warrior2\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\w.png\"\n",
    "}\n",
    "\n",
    "pose_advantages = {\n",
    "    \"downdog\": \"Stretches the shoulders, hamstrings, calves, arches, and hands. Strengthens the arms and legs. Relieves back pain.\",\n",
    "    \"goddess\": \"Strengthens the legs and core muscles. Opens the hips and chest. Improves posture.\",\n",
    "    \"plank\": \"Strengthens the core muscles, shoulders, arms, and wrists. Improves posture and balance.\",\n",
    "    \"tree\": \"Improves balance and concentration. Strengthens the legs, ankles, and core muscles.\",\n",
    "    \"warrior2\": \"Strengthens the legs, ankles, and arms. Stretches the groins, chest, lungs, and shoulders.\"\n",
    "}\n",
    "\n",
    "# Function to preprocess the frame before prediction\n",
    "def preprocess_frame(frame):\n",
    "    processed_frame = cv2.resize(frame, (48, 48))  # Resize frame to match model input size\n",
    "    processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    processed_frame = np.expand_dims(processed_frame, axis=-1)  # Add channel dimension\n",
    "    processed_frame = np.expand_dims(processed_frame, axis=0)  # Add batch dimension\n",
    "    processed_frame = processed_frame / 255.0  # Normalize pixel values\n",
    "    return processed_frame\n",
    "\n",
    "# Function to process video frames and perform yoga pose recognition\n",
    "def recognize_pose():\n",
    "    cap = cv2.VideoCapture(r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\gd.mp4\")  # Capture video from webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Can't open camera\")\n",
    "        return\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Read a frame from the video stream\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame to RGB\n",
    "        frame = cv2.resize(frame, (400, 400))  # Resize frame\n",
    "\n",
    "        # Preprocess the frame for prediction\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "\n",
    "        # Perform yoga pose prediction\n",
    "        prediction = yoga_model.predict(processed_frame)\n",
    "        max_index = np.argmax(prediction)\n",
    "        predicted_pose = pose_dict[max_index]\n",
    "\n",
    "        # Display the frame with the predicted pose label\n",
    "        display_frame(frame, predicted_pose)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to display the frame with the predicted pose label, image, and advantages\n",
    "def display_frame(frame, predicted_pose):\n",
    "    frame = Image.fromarray(frame)  # Convert frame to PIL Image\n",
    "    frame = ImageTk.PhotoImage(image=frame)  # Convert PIL Image to Tkinter PhotoImage\n",
    "    \n",
    "    # Display input image and information on the left side\n",
    "    input_label.config(image=frame)\n",
    "    input_label.image = frame  # Keep reference to the image object to prevent garbage collection\n",
    "\n",
    "    # Load and display image of the predicted pose on the right side\n",
    "    pose_image_path = pose_images.get(predicted_pose, \"path/to/default_image.jpg\")\n",
    "    pose_img = Image.open(pose_image_path)\n",
    "    pose_img = pose_img.resize((400, 400), Image.ANTIALIAS)\n",
    "    pose_img = ImageTk.PhotoImage(pose_img)\n",
    "    pose_label.config(image=pose_img)\n",
    "    pose_label.image = pose_img  # Keep reference to the image object to prevent garbage collection\n",
    "\n",
    "    # Display pose name and advantages on the right side\n",
    "    pose_label_text.config(text=f\"Predicted Pose: {predicted_pose}\\n\\nAdvantages:\\n{pose_advantages.get(predicted_pose, '')}\")\n",
    "\n",
    "# Initialize Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Yoga Pose Recognition\")\n",
    "root.geometry(\"1000x600\")\n",
    "root['bg'] = 'white'\n",
    "\n",
    "# Create frame for left side (input image and information)\n",
    "left_frame = tk.Frame(root)\n",
    "left_frame.pack(side=tk.LEFT, padx=100, pady=10)\n",
    "\n",
    "# Create frame for right side (output image, pose name, and advantages)\n",
    "right_frame = tk.Frame(root)\n",
    "right_frame.pack(side=tk.RIGHT, padx=100, pady=10)\n",
    "\n",
    "\n",
    "# Create label to display input image\n",
    "input_label = tk.Label(left_frame)\n",
    "input_label.pack()\n",
    "\n",
    "# Create label to display output image\n",
    "pose_label = tk.Label(right_frame)\n",
    "pose_label.pack()\n",
    "\n",
    "# Create label to display predicted pose and advantages\n",
    "pose_label_text = tk.Label(right_frame, font=(\"Arial\", 12), justify=CENTER)\n",
    "pose_label_text.pack()\n",
    "\n",
    "# Start a new thread to run the yoga pose recognition function\n",
    "threading.Thread(target=recognize_pose).start()\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c1ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49ff70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f241d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29f176d2",
   "metadata": {},
   "source": [
    "MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b18e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79e41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22dc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072de33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f485a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import threading\n",
    "\n",
    "# Load the trained model for yoga pose classification\n",
    "yoga_model = Sequential()\n",
    "yoga_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "yoga_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Dropout(0.25))\n",
    "yoga_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "yoga_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "yoga_model.add(Dropout(0.25))\n",
    "yoga_model.add(Flatten())\n",
    "yoga_model.add(Dense(1024, activation='relu'))\n",
    "yoga_model.add(Dropout(0.5))\n",
    "yoga_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "yoga_model.load_weights('yoga_model.h5')  # Load pre-trained weights\n",
    "\n",
    "pose_dict = {0: \"downdog\", 1: \"goddess\", 2: \"plank\", 3: \"tree\", 4: \"warrior2\"}\n",
    "\n",
    "pose_images = {\n",
    "    \"downdog\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\dd.png\",\n",
    "    \"goddess\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\gd.png\",\n",
    "    \"plank\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\plank.jpg\",\n",
    "    \"tree\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\treee.png\",\n",
    "    \"warrior2\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\op\\w.png\"\n",
    "}\n",
    "\n",
    "pose_advantages = {\n",
    "    \"downdog\": \"Stretches the shoulders, hamstrings, calves, arches, and hands. Strengthens the arms and legs. Relieves back pain.\",\n",
    "    \"goddess\": \"Strengthens the legs and core muscles. Opens the hips and chest. Improves posture.\",\n",
    "    \"plank\": \"Strengthens the core muscles, shoulders, arms, and wrists. Improves posture and balance.\",\n",
    "    \"tree\": \"Improves balance and concentration. Strengthens the legs, ankles, and core muscles.\",\n",
    "    \"warrior2\": \"Strengthens the legs, ankles, and arms. Stretches the groins, chest, lungs, and shoulders.\"\n",
    "}\n",
    "\n",
    "# Define a dictionary mapping each pose to a set of reference images\n",
    "reference_images = {\n",
    "    \"downdog\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\how-to\\how-dd.png\",\n",
    "    \"goddess\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\how-to\\how-g.png\",\n",
    "    \"plank\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\how-to\\how-p.png\",\n",
    "    \"tree\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\how-to\\how-t.png\",\n",
    "    \"warrior2\": r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\how-to\\how-w.png\"\n",
    "}\n",
    "\n",
    "# Function to preprocess the frame before prediction\n",
    "def preprocess_frame(frame):\n",
    "    processed_frame = cv2.resize(frame, (48, 48))  # Resize frame to match model input size\n",
    "    processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    processed_frame = np.expand_dims(processed_frame, axis=-1)  # Add channel dimension\n",
    "    processed_frame = np.expand_dims(processed_frame, axis=0)  # Add batch dimension\n",
    "    processed_frame = processed_frame / 255.0  # Normalize pixel values\n",
    "    return processed_frame\n",
    "\n",
    "# Function to suggest the correct pose if the detected pose is improper\n",
    "def suggest_correct_pose(predicted_pose):\n",
    "    reference_image_path = reference_images.get(predicted_pose)\n",
    "    if reference_image_path:\n",
    "        img = Image.open(reference_image_path)\n",
    "        img = img.resize((500, 500), Image.ANTIALIAS)\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        reference_label.configure(image=img)\n",
    "        reference_label.image = img\n",
    "    else:\n",
    "        print(\"Reference image not found for pose:\", predicted_pose)\n",
    "\n",
    "# Function to process video frames and perform yoga pose recognition\n",
    "def recognize_pose():\n",
    "    cap = cv2.VideoCapture(r\"C:\\Users\\vshas\\OneDrive\\Desktop\\Major\\DATASET\\wrr.mp4\")  # Video path\n",
    "    if not cap.isOpened():\n",
    "        print(\"Can't open camera\")\n",
    "        return\n",
    "\n",
    "    def update_frame():\n",
    "        ret, frame = cap.read()  # Read a frame from the video stream\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame to RGB\n",
    "            frame = cv2.resize(frame, (450, 500))  # Resize frame\n",
    "\n",
    "            # Preprocess the frame for prediction\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "\n",
    "            # Perform yoga pose prediction\n",
    "            prediction = yoga_model.predict(processed_frame)\n",
    "            max_index = np.argmax(prediction)\n",
    "            predicted_pose = pose_dict.get(max_index)\n",
    "            if predicted_pose:\n",
    "                display_frame(frame, predicted_pose)\n",
    "            else:\n",
    "                print(\"Unknown pose index:\", max_index)\n",
    "\n",
    "        # Schedule the next update after 10 milliseconds\n",
    "        root.after(10, update_frame)\n",
    "\n",
    "    # Start updating the frame\n",
    "    update_frame()\n",
    "\n",
    "def display_frame(frame, predicted_pose):\n",
    "    frame = Image.fromarray(frame)  # Convert frame to PIL Image\n",
    "    frame = ImageTk.PhotoImage(image=frame)  # Convert PIL Image to Tkinter PhotoImage\n",
    "\n",
    "    # Display input image and information on the left side\n",
    "    input_label.configure(image=frame)\n",
    "    input_label.image = frame\n",
    "\n",
    "    # Display output image and pose information in the center\n",
    "    pose_image_path = pose_images.get(predicted_pose)\n",
    "    if pose_image_path:\n",
    "        pose_img = Image.open(pose_image_path)\n",
    "        pose_img = pose_img.resize((400, 400), Image.ANTIALIAS)\n",
    "        pose_img = ImageTk.PhotoImage(pose_img)\n",
    "        pose_label.configure(image=pose_img)\n",
    "        pose_label.image = pose_img\n",
    "    else:\n",
    "        print(\"Image not found for pose:\", predicted_pose)\n",
    "\n",
    "    pose_label_text.config\n",
    "    \n",
    "    \n",
    "        # Update pose label text\n",
    "    pose_label_text.config(text=f\"Predicted Pose: {predicted_pose}\\n\\nAdvantages:\\n{pose_advantages.get(predicted_pose, '')}\")\n",
    "\n",
    "    # Suggest the correct pose if the detected pose is improper\n",
    "    suggest_correct_pose(predicted_pose)\n",
    "\n",
    "# Initialize Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Yoga Pose Recognition\")\n",
    "root.geometry(\"1200x400\")\n",
    "root['bg'] = 'white'\n",
    "\n",
    "# Create frame for left side (input image and information)\n",
    "left_frame = tk.Frame(root)\n",
    "left_frame.pack(side=tk.LEFT, padx=30, pady=10)\n",
    "\n",
    "# Create frame for center (output image, pose name, and advantages)\n",
    "center_frame = tk.Frame(root)\n",
    "center_frame.pack(side=tk.LEFT, padx=30, pady=10)\n",
    "\n",
    "# Create frame for right side (reference image)\n",
    "right_frame = tk.Frame(root)\n",
    "right_frame.pack(side=tk.LEFT, padx=30, pady=10)\n",
    "\n",
    "# Create label to display input image\n",
    "input_label = tk.Label(left_frame)\n",
    "input_label.pack()\n",
    "\n",
    "# Create label to display output image\n",
    "pose_label = tk.Label(center_frame)\n",
    "pose_label.pack()\n",
    "\n",
    "# Create label to display predicted pose and advantages\n",
    "pose_label_text = tk.Label(center_frame, font=(\"Arial\", 12), justify=LEFT, wraplength=400)\n",
    "pose_label_text.pack()\n",
    "\n",
    "# Create label to display reference image\n",
    "reference_label = tk.Label(right_frame)\n",
    "reference_label.pack()\n",
    "\n",
    "# Start a new thread to run the recognize_pose function\n",
    "threading.Thread(target=recognize_pose).start()\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e093e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
